<!doctype html>
<html lang="ja">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Hybrid AI Voice Assistant (è‡ªå‹•é€£ç¶šèªè­˜ + VAD)</title>
<style>
    :root{--accent:#00ffff;--accent-2:#00ffaa;--bg:#0f0f0f}
    html,body{height:100%;margin:0;background:var(--bg);color:#fff;font-family:Segoe UI,system-ui,Arial}
    canvas{position:fixed;inset:0;z-index:0}
    
    /* ğŸ’¡ UIã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³/ã‚¢ã‚¦ãƒˆã¨ã‚¿ãƒƒãƒ—é ˜åŸŸã®ã‚¹ã‚¿ã‚¤ãƒ« */
    #ui{
        position:absolute;left:50%;bottom:5%;transform:translateX(-50%);z-index:10;width:min(980px,94vw);
        opacity: 1; /* åˆæœŸè¡¨ç¤º */
        transition: opacity 0.5s ease-in-out; /* ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š */
    }
    /* ç”»é¢å…¨ä½“ã‚’ã‚¿ãƒƒãƒ—é ˜åŸŸã¨ã—ã¦è¨­å®š */
    #tapArea {
        position: fixed;
        inset: 0;
        z-index: 5; /* UIã®ä¸‹ã€ã‚­ãƒ£ãƒ³ãƒã‚¹ã®ä¸Šã«é…ç½® */
    }

    #status-area{padding:14px 20px;border-radius:12px;background:rgba(0,0,0,0.45);box-shadow:0 0 20px #00ffff55;color:var(--accent);font-weight:700}
    #controls{display:flex;gap:12px;margin-top:10px;align-items:center}
    #messageInput{flex:1;padding:12px 14px;border-radius:10px;border:1px solid rgba(0,255,255,0.15);background:rgba(255,255,255,0.03);color:#fff;font-size:16px}
    button{padding:10px 14px;border-radius:10px;border:none;cursor:pointer;font-weight:700}
    #micBtn{background:var(--accent-2);color:#000} /* ã“ã®ãƒœã‚¿ãƒ³ã¯æ©Ÿèƒ½çš„ã«ã¯ä½¿ã‚ãªã„ãŒã€CSSã¯æ®‹ã™ */
    #resetBtn{background:var(--accent);color:#000}
    #modeIndicator{padding:8px 10px;border-radius:8px;background:#00000044;font-size:0.9rem}
    #subtext{margin-top:8px;color:#bfeeff}
    .active{box-shadow:0 0 20px #ff5555}
    #transcript{margin-top:12px;padding:12px;border-radius:10px;background:rgba(255,255,255,0.02);min-height:48px;font-size:16px}
</style>
</head>
<body>
<canvas id="waveCanvas"></canvas>

<div id="tapArea"></div>

<div id="ui">
    <div id="status-area">Initializing...</div>
    <div id="controls">
        <input id="messageInput" placeholder="è©±ã—ã‹ã‘ã¦ãã ã•ã„..." disabled> <button id="resetBtn">ãƒªã‚»ãƒƒãƒˆ</button>
        <div id="modeIndicator">--</div>
    </div>
    <div id="subtext">é€£ç¶šèªè­˜ãƒ¢ãƒ¼ãƒ‰ï¼ˆVADã«ã‚ˆã‚‹è‡ªå‹•ã‚¹ã‚¿ãƒ¼ãƒˆ/ã‚¹ãƒˆãƒƒãƒ—ï¼‰</div>
    <div id="transcript"></div>
</div><script>
/* ---------- Config ---------- */
const WHISPER_WS_URL = 'ws://localhost:8765'; // change if server is remote
const VAD_SILENCE_MS = 1500; // ğŸ’¡ ã‚µã‚¤ãƒ¬ãƒ³ã‚¹åˆ¤å®šæ™‚é–“ (é•·ã‚ã«è¨­å®šã—ã€è‡ªå‹•ã‚¹ãƒˆãƒƒãƒ—ã®å½¹å‰²ã‚’æŒãŸã›ã‚‹)
const AUDIO_CHUNK_MS = 300; // chunk length sent to server (ms)
const ENABLE_FALLBACK = true; // use browser SpeechRecognition if server unreachable

/* ---------- DOM ---------- */
const statusArea = document.getElementById('status-area');
const resetBtn = document.getElementById('resetBtn');
const input = document.getElementById('messageInput');
const modeIndicator = document.getElementById('modeIndicator');
const transcriptBox = document.getElementById('transcript');
const ui = document.getElementById('ui'); // UIã‚³ãƒ³ãƒ†ãƒŠ
const tapArea = document.getElementById('tapArea'); // ã‚¿ãƒƒãƒ—é ˜åŸŸ

/* ---------- Audio / Waveform ---------- */
const canvas = document.getElementById('waveCanvas');
const ctx = canvas.getContext('2d');
function resizeCanvas(){canvas.width = innerWidth; canvas.height = innerHeight}
window.addEventListener('resize', resizeCanvas); resizeCanvas();

let audioContext, analyser, mediaStream, sourceNode, processorNode;
let isRecording = false;
let ws=null, wsAvailable=false;
let useServer=false;
let recognition=null; // browser fallback

/* VAD helper */
let lastSpokenTime = 0;
let vadSilenceTimer = null;
function nowMs(){return performance.now()}

/* Simple energy VAD */
function isLoud(buffer, threshold=0.01){
    let sum=0; for(let i=0;i<buffer.length;i++){ let v=buffer[i]; sum += v*v }
    let rms = Math.sqrt(sum / buffer.length);
    return rms > threshold;
}

/* Waveform draw */
let waveformData = new Float32Array(1024);
// Waveformã®æç”»ãƒ­ã‚¸ãƒƒã‚¯ã¯init()å†…ã®loopCanvas()ã«ç§»è­²ã•ã‚Œã€åˆ†æãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å‹•çš„ã«å¤‰åŒ–ã™ã‚‹

/* ---------- WebSocket (Whisper) client ---------- */
function connectWS(){
    status('Connecting to Whisper server...');
    ws = new WebSocket(WHISPER_WS_URL);
    ws.binaryType = 'arraybuffer';
    ws.onopen = ()=>{ wsAvailable=true; status('Whisper server connected'); modeIndicator.textContent='mode: server (Whisper)'; useServer=true };
    ws.onmessage = (ev)=>{
        // assume text messages are partial/final transcripts
        const text = typeof ev.data === 'string' ? ev.data : new TextDecoder().decode(ev.data);
        // server may send JSON: {type:'partial'|'final', text:'...'}
        try{
            const j = JSON.parse(text);
            if(j.type === 'partial'){
                input.value = j.text; transcriptBox.textContent = j.text;
            } else if(j.type==='final'){
                input.value = j.text; transcriptBox.textContent = j.text; speak(j.text);
            } else {
                input.value = j.text; transcriptBox.textContent = j.text;
            }
        }catch(e){
            // plain text fallback
            input.value = text; transcriptBox.textContent = text; speak(text);
        }
    };
    ws.onerror = (e)=>{ console.warn('ws err',e); wsAvailable=false; useServer=false; if(ENABLE_FALLBACK) status('Whisper server unreachable. Will use browser STT fallback'); };
    ws.onclose = ()=>{ wsAvailable=false; useServer=false; /* stopRecording(); */ status('Whisper server closed'); modeIndicator.textContent='mode: fallback'; };
}

/* ---------- Audio capture & streaming (ä¿®æ­£: VADã«ã‚ˆã‚‹è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã¨ãƒªã‚¹ã‚¿ãƒ¼ãƒˆ) ---------- */
async function startRecording(){
    if(isRecording) return;
    isRecording = true; 
    
    try{
        mediaStream = await navigator.mediaDevices.getUserMedia({audio:true});
    }catch(err){ status('ãƒã‚¤ã‚¯ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ'); console.error(err); return }

    audioContext = new (window.AudioContext || window.webkitAudioContext)({sampleRate: 16000});
    sourceNode = audioContext.createMediaStreamSource(mediaStream);
    analyser = audioContext.createAnalyser(); analyser.fftSize = 2048;
    sourceNode.connect(analyser);
    processorNode = audioContext.createScriptProcessor(4096,1,1);
    sourceNode.connect(processorNode); processorNode.connect(audioContext.destination);

    const bufferQueue = [];
    let chunkMillis = AUDIO_CHUNK_MS;
    let sampleRate = audioContext.sampleRate;
    let samplesPerChunk = Math.floor(sampleRate * (chunkMillis/1000));

    // ğŸ’¡ éŒ²éŸ³é–‹å§‹æ™‚ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–° (è‡ªå‹•èªè­˜ãƒ¢ãƒ¼ãƒ‰)
    status(useServer ? 'Listening (Whisper Server)...' : 'Listening (Browser STT)...');
    lastSpokenTime = nowMs(); // ã‚¿ã‚¤ãƒãƒ¼ãƒªã‚»ãƒƒãƒˆ

    processorNode.onaudioprocess = (evt)=>{
        const chData = evt.inputBuffer.getChannelData(0);
        const chunk = new Float32Array(chData.length); chunk.set(chData);
        bufferQueue.push(chunk);

        if(isLoud(chData)){
            lastSpokenTime = nowMs();
        }

        let totalSamples = bufferQueue.reduce((a,b)=>a+b.length,0);
        if(totalSamples >= samplesPerChunk){
            const out = new Float32Array(samplesPerChunk);
            let offset=0;
            while(offset < samplesPerChunk){
                const piece = bufferQueue.shift();
                const needed = Math.min(piece.length, samplesPerChunk - offset);
                out.set(piece.subarray(0,needed), offset);
                if(needed < piece.length){
                    bufferQueue.unshift(piece.subarray(needed));
                }
                offset += needed;
            }

            if(useServer && ws && ws.readyState===1){
                // send audio chunk to Whisper server
                const pcm16 = floatTo16BitPCM(out);
                ws.send(pcm16.buffer);
            }
            
            // ğŸ’¡ VADã«ã‚ˆã‚‹ã‚µã‚¤ãƒ¬ãƒ³ã‚¹æ¤œå‡ºã¨ã‚³ãƒŸãƒƒãƒˆ/ãƒªã‚¹ã‚¿ãƒ¼ãƒˆ
            if(nowMs() - lastSpokenTime > VAD_SILENCE_MS){
                if(useServer && ws && ws.readyState===1){ 
                    try{ ws.send(JSON.stringify({type:'commit'})) }catch(e){} // ã‚µãƒ¼ãƒãƒ¼ã«çµ‚äº†ã‚’é€šçŸ¥
                }
                
                // ãƒ–ãƒ©ã‚¦ã‚¶èªè­˜ã®å ´åˆã¯é€£ç¶šãƒ¢ãƒ¼ãƒ‰ãªã®ã§ã€ã“ã“ã§ã¯ç‰¹ã«ä½•ã‚‚ã—ãªã„ï¼ˆonendã‚’å¾…ã¤ï¼‰
                // ğŸ’¡ é€£ç¶šèªè­˜ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€éŸ³å£°ã®åŒºåˆ‡ã‚Šã”ã¨ã«å†…å®¹ã‚’ç¢ºå®šã•ã›ã€éŒ²éŸ³ã‚’å†é–‹ã™ã‚‹ï¼ˆã¾ãŸã¯ç¶™ç¶šã•ã›ã‚‹ï¼‰ã®ãŒç†æƒ³
                // ã“ã“ã§ã¯ç°¡å˜åŒ–ã®ãŸã‚ã€VADã§ã‚µã‚¤ãƒ¬ãƒ³ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¦ã‚‚éŒ²éŸ³è‡ªä½“ã¯æ­¢ã‚ãšã€ã‚µãƒ¼ãƒãƒ¼ã«ã‚³ãƒŸãƒƒãƒˆä¿¡å·ã‚’é€ã‚‹ã®ã¿ã¨ã—ã¾ã™ã€‚
                lastSpokenTime = nowMs(); // ã‚¿ã‚¤ãƒãƒ¼ãƒªã‚»ãƒƒãƒˆ
                
                // ğŸ’¡ ã‚µã‚¤ãƒ¬ãƒ³ã‚¹æ™‚ã®UIãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                status(useServer ? 'Listening (Whisper Server): Silence' : 'Listening (Browser STT): Silence');
            } else {
                 status(useServer ? 'Listening (Whisper Server): Speaking' : 'Listening (Browser STT): Speaking');
            }
        }
    };

    // start fallback SpeechRecognition if needed and server unavailable
    if(ENABLE_FALLBACK && !useServer) startBrowserRecognition();
}

function stopRecording(){
    if(!isRecording) return;
    isRecording=false; 
    try{ processorNode && processorNode.disconnect(); sourceNode && sourceNode.disconnect(); analyser && (analyser.disconnect && analyser.disconnect()); }
    catch(e){}
    mediaStream && mediaStream.getTracks().forEach(t=>t.stop());
    audioContext && audioContext.close(); audioContext=null; analyser=null; processorNode=null; sourceNode=null;
    status('Stopped');
    stopBrowserRecognition();
}

function floatTo16BitPCM(float32Array){
    const l = float32Array.length; const buf = new ArrayBuffer(l*2); const view = new DataView(buf);
    let offset=0; for(let i=0;i<l;i++){ let s=Math.max(-1,Math.min(1,float32Array[i])); view.setInt16(offset, s<0? s*0x8000 : s*0x7FFF, true); offset+=2 }
    return new Int16Array(buf);
}

/* ---------- Browser SpeechRecognition fallback ---------- */
function startBrowserRecognition(){
    if(!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)){
        status('ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°èªè­˜ãŒæœªå¯¾å¿œã§ã™'); return; }
    const SpeechRec = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognition = new SpeechRec(); recognition.lang='ja-JP'; recognition.interimResults=true; recognition.continuous=true;
    recognition.onstart = ()=>{ status('Listening (Browser STT)...'); modeIndicator.textContent='mode: browser STT'; }
    recognition.onresult = (ev)=>{
        let interim=''; let final='';
        for(let i=0;i<ev.results.length;i++){ const t=ev.results[i][0].transcript; ev.results[i].isFinal? final+=t: interim+=t }
        input.value = final || interim; transcriptBox.textContent = input.value;
        if(final) speak(final);
    };
    recognition.onerror = (e)=>{ console.warn('recog err',e); };
    recognition.onend = ()=>{ /* recognition.start(); // continuous=trueã§è‡ªå‹•å†é–‹ã™ã‚‹ã¯ãš */ };
    recognition.start();
}
function stopBrowserRecognition(){ if(recognition){ try{ recognition.stop() }catch(e){} recognition=null } }

/* ---------- TTS ---------- */
const synth = window.speechSynthesis;
let lastSpokenText = '';
function speak(text){ 
    if(!text || text===lastSpokenText) return; 
    lastSpokenText = text; 
    if(synth.speaking) synth.cancel(); 
    const u = new SpeechSynthesisUtterance(text); 
    u.lang='ja-JP'; 
    u.rate=1.0; 
    u.onstart=()=>{ status('Speaking...'); }; 
    u.onend=()=>{ status('Listening...'); }; // èª­ã¿ä¸Šã’çµ‚äº†å¾Œã€ãƒªã‚¹ãƒ‹ãƒ³ã‚°çŠ¶æ…‹ã«æˆ»ã‚‹
    synth.speak(u); 
}

/* ---------- UI helpers ---------- */
function status(msg){ statusArea.textContent = msg }

/* ---------- UI ãƒˆã‚°ãƒ«æ©Ÿèƒ½ (ç”»é¢ã‚¿ãƒƒãƒ—) ---------- */
let uiVisible = true;
tapArea.addEventListener('click', (e) => {
    // è¨˜å…¥æ¬„ã‚„ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã¸ã®ã‚¿ãƒƒãƒ—ã¯ç„¡è¦–ã™ã‚‹
    if (e.target.closest('#controls') || e.target.closest('#transcript')) {
        return;
    }

    uiVisible = !uiVisible;
    if (uiVisible) {
        ui.style.opacity = 1; // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
    } else {
        ui.style.opacity = 0; // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ
    }
});


/* ---------- Controls ---------- */
resetBtn.addEventListener('click', ()=>{ 
    input.value=''; transcriptBox.textContent=''; 
    if(synth.speaking) synth.cancel(); 
    lastSpokenText = '';
    status('Reset'); 
});


/* ---------- Start-up ---------- */
(function init(){
    // 1. WebSocketæ¥ç¶šãƒã‚§ãƒƒã‚¯ã¨ãƒ¢ãƒ¼ãƒ‰è¨­å®š
    status('Initializing audio...');
    new Promise(resolve => {
        try{ 
            const p = new WebSocket(WHISPER_WS_URL); 
            p.onopen=()=>{ 
                p.close(); 
                wsAvailable = true;
                useServer = true;
                status('Whisper server reachable'); 
                modeIndicator.textContent='mode: server (available)'; 
                resolve();
            }; 
            p.onerror=()=>{ 
                wsAvailable = false;
                useServer = false;
                modeIndicator.textContent='mode: fallback (no server)'; 
                status('Whisper server not reachable - will use browser STT'); 
                resolve();
            }; 
        } catch(e){ 
            wsAvailable = false;
            useServer = false;
            modeIndicator.textContent='mode: fallback'; 
            status('ready'); 
            resolve();
        }
    }).then(() => {
        // 2. èµ·å‹•æ™‚ã«è‡ªå‹•ã§èªè­˜ã‚’é–‹å§‹
        startRecording();
    });

    // 3. Waveformã®è¦–è¦šãƒ«ãƒ¼ãƒ—
    (function loopCanvas(){
        if(analyser){
            const bufferLen = analyser.fftSize; const data = new Float32Array(bufferLen);
            analyser.getFloatTimeDomainData(data);
            ctx.clearRect(0,0,canvas.width,canvas.height);
            ctx.beginPath();
            const mid = canvas.height*0.55; const step = canvas.width / bufferLen;
            for(let i=0;i<bufferLen;i++){ 
                // ğŸ’¡ æ³¢å½¢ã‚’ã‚ˆã‚Šãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã«
                const amp = isRecording ? 0.9 : 0.4;
                const y=mid + data[i]*mid*amp; 
                if(i===0)ctx.moveTo(x,y);else ctx.lineTo(x,y) 
            }
            ctx.strokeStyle='rgba(0,230,255,0.9)'; ctx.lineWidth=2; ctx.stroke();
        }
        requestAnimationFrame(loopCanvas);
    })();
})();

</script></body>
</html>



<!doctype html>
<html lang="ja">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>ã‚¤ãƒã‚¸ãƒ³ã•ã‚“ AIã€€ãƒ–ãƒãƒ¥ãƒ–ãƒãƒ¥ãƒãƒ³ã¾ãŸã¯ã¶ã¤ã¶ã¤ãƒãƒ³ Voice Assistant (è‡ªå‹•é€£ç¶šèªè­˜ + VAD)</title>
<style>
/* CSSéƒ¨åˆ†ã¯å¤‰æ›´ãªã— */
    :root{--accent:#00ffff;--accent-2:#00ffaa;--bg:#0f0f0f}
    html,body{height:100%;margin:0;background:var(--bg);color:#fff;font-family:Segoe UI,system-ui,Arial}
    canvas{position:fixed;inset:0;z-index:0}
    
    /* ğŸ’¡ UIã®ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³/ã‚¢ã‚¦ãƒˆã¨ã‚¿ãƒƒãƒ—é ˜åŸŸã®ã‚¹ã‚¿ã‚¤ãƒ« */
    #ui{
        position:absolute;left:50%;bottom:5%;transform:translateX(-50%);z-index:10;width:min(980px,94vw);
        opacity: 1; /* åˆæœŸè¡¨ç¤º */
        transition: opacity 0.5s ease-in-out; /* ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š */
    }
    /* ç”»é¢å…¨ä½“ã‚’ã‚¿ãƒƒãƒ—é ˜åŸŸã¨ã—ã¦è¨­å®š */
    #tapArea {
        position: fixed;
        inset: 0;
        z-index: 5; /* UIã®ä¸‹ã€ã‚­ãƒ£ãƒ³ãƒã‚¹ã®ä¸Šã«é…ç½® */
    }

    #status-area{padding:14px 20px;border-radius:12px;background:rgba(0,0,0,0.45);box-shadow:0 0 20px #00ffff55;color:var(--accent);font-weight:700}
    #controls{display:flex;gap:12px;margin-top:10px;align-items:center}
    #messageInput{flex:1;padding:12px 14px;border-radius:10px;border:1px solid rgba(0,255,255,0.15);background:rgba(255,255,255,0.03);color:#fff;font-size:16px}
    button{padding:10px 14px;border-radius:10px;border:none;cursor:pointer;font-weight:700}
    #micBtn{background:var(--accent-2);color:#000} /* ã“ã®ãƒœã‚¿ãƒ³ã¯æ©Ÿèƒ½çš„ã«ã¯ä½¿ã‚ãªã„ãŒã€CSSã¯æ®‹ã™ */
    #resetBtn{background:var(--accent);color:#000}
    #modeIndicator{padding:8px 10px;border-radius:8px;background:#00000044;font-size:0.9rem}
    #subtext{margin-top:8px;color:#bfeeff}
    .active{box-shadow:0 0 20px #ff5555}
    #transcript{margin-top:12px;padding:12px;border-radius:10px;background:rgba(255,255,255,0.02);min-height:48px;font-size:16px}
</style>
</head>
<body>
<canvas id="waveCanvas"></canvas>

<div id="tapArea"></div>

<div id="ui">
    <div id="status-area">Initializing...</div>
    <div id="controls">
        <input id="messageInput" placeholder="è©±ã—ã‹ã‘ã¦ãã ã•ã„..." disabled> <button id="resetBtn">ãƒªã‚»ãƒƒãƒˆ</button>
        <div id="modeIndicator">--</div>
    </div>
    <div id="subtext">é€£ç¶šèªè­˜ãƒ¢ãƒ¼ãƒ‰ï¼ˆVADã«ã‚ˆã‚‹è‡ªå‹•ã‚¹ã‚¿ãƒ¼ãƒˆ/ã‚¹ãƒˆãƒƒãƒ—ï¼‰</div>
    <div id="transcript"></div>
</div><script>
// Canvasç’°å¢ƒã§ã¯è‡ªå‹•ã§APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã™
const API_KEY = ""; 
const GEMINI_MODEL = 'gemini-2.5-flash-preview-09-2025';

/* ---------- DOM ---------- */
const statusArea = document.getElementById('status-area');
const resetBtn = document.getElementById('resetBtn');
const input = document.getElementById('messageInput');
const modeIndicator = document.getElementById('modeIndicator');
const transcriptBox = document.getElementById('transcript');
const ui = document.getElementById('ui'); 
const tapArea = document.getElementById('tapArea'); 

/* ---------- Audio / Waveform ---------- */
const canvas = document.getElementById('waveCanvas');
const ctx = canvas.getContext('2d');
function resizeCanvas(){canvas.width = innerWidth; canvas.height = innerHeight}
window.addEventListener('resize', resizeCanvas); resizeCanvas();

let audioContext, analyser, mediaStream;
let isRecording = false; // éŸ³å£°èªè­˜ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ã©ã†ã‹
let recognition = null; // Web Speech API SpeechRecognition Object

/* ---------- UI helpers ---------- */
function status(msg){ statusArea.textContent = msg }

/* ---------- Speech Recognition (Browser STT) & Audio Init ---------- */

/**
 * Web Speech APIã«ã‚ˆã‚‹éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚
 * ã“ã®é–¢æ•°ã¯ã€ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ãŒæˆåŠŸã—ã€audioContextãŒæº–å‚™ã§ãã¦ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚
 */
function startBrowserRecognition() {
Â  Â  // æ—¢ã«å®Ÿè¡Œä¸­ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
Â  Â  if (isRecording) return;
Â  Â  
Â  Â  if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
Â  Â  Â  Â  status('Error: Speech Recognition not supported in this browser.');
Â  Â  Â  Â  return;
Â  Â  }

Â  Â  // æ—¢å­˜ã®èªè­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚¯ãƒªã‚¢
Â  Â  if (recognition) {
Â  Â  Â  Â  recognition.stop();
Â  Â  Â  Â  recognition = null;
Â  Â  }

Â  Â  recognition = new (window.webkitSpeechRecognition || window.SpeechRecognition)();

Â  Â  // é€£ç¶šèªè­˜ãƒ¢ãƒ¼ãƒ‰ (ç™ºè©±ã®åˆ‡ã‚Œç›®ã§è‡ªå‹•åœæ­¢)
Â  Â  recognition.continuous = false; 
Â  Â  recognition.interimResults = true; 
Â  Â  recognition.lang = 'ja-JP';

Â  Â  recognition.onstart = () => {
Â  Â  Â  Â  isRecording = true;
Â  Â  Â  Â  status('Listening...');
Â  Â  Â  Â  input.value = 'è©±ã—ã¦ã„ã¾ã™...';
Â  Â  };

Â  Â  recognition.onresult = (event) => {
Â  Â  Â  Â  let interimTranscript = '';
Â  Â  Â  Â  let finalTranscript = '';

Â  Â  Â  Â  for (let i = event.resultIndex; i < event.results.length; ++i) {
Â  Â  Â  Â  Â  Â  if (event.results[i].isFinal) {
Â  Â  Â  Â  Â  Â  Â  Â  finalTranscript += event.results[i][0].transcript;
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  interimTranscript += event.results[i][0].transcript;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  transcriptBox.textContent = finalTranscript || interimTranscript;
Â  Â  Â  Â  input.value = finalTranscript || interimTranscript;
Â  Â  };

Â  Â  // ğŸ’¡ ç™ºè©±çµ‚äº†ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•å†ã‚¹ã‚¿ãƒ¼ãƒˆãƒ­ã‚¸ãƒƒã‚¯
Â  Â  const restartRecognition = () => {
Â  Â  Â  Â  status('Recognition stopped. Restarting...');
Â  Â  Â  Â  setTimeout(() => {
Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  recognition.start();
Â  Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  Â  Â  // æ—¢ã«å®Ÿè¡Œä¸­ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
Â  Â  Â  Â  Â  Â  Â  Â  if (e.name !== 'InvalidStateError') {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.warn('Recognition start failed:', e);
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }, 500); 
Â  Â  };
Â  Â  
Â  Â  recognition.onend = () => {
Â  Â  Â  Â  isRecording = false;
Â  Â  Â  Â  
Â  Â  Â  Â  const finalPrompt = transcriptBox.textContent.trim();
Â  Â  Â  Â  
Â  Â  Â  Â  if (finalPrompt && finalPrompt.length > 1 && !finalPrompt.startsWith("AIå¿œç­”:")) { // çŸ­ã™ãã‚‹ç™ºè©±ã‚„AIå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’ç„¡è¦–
Â  Â  Â  Â  Â  Â  status('Processing response...');
Â  Â  Â  Â  Â  Â  processRecognitionResult(finalPrompt).finally(() => {
Â  Â  Â  Â  Â  Â  Â  Â  restartRecognition(); // AIå¿œç­”å¾Œã«å†ã‚¹ã‚¿ãƒ¼ãƒˆ
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  // èªè­˜çµæœãŒãªã„å ´åˆã¯å³åº§ã«å†ã‚¹ã‚¿ãƒ¼ãƒˆ
Â  Â  Â  Â  Â  Â  transcriptBox.textContent = '';
Â  Â  Â  Â  Â  Â  input.value = 'è©±ã—ã‹ã‘ã¦ãã ã•ã„...';
Â  Â  Â  Â  Â  Â  restartRecognition();
Â  Â  Â  Â  }
Â  Â  };

Â  Â  recognition.onerror = (event) => {
Â  Â  Â  Â  isRecording = false;
Â  Â  Â  Â  console.error('Speech Recognition Error:', event.error);
Â  Â  Â  Â  
Â  Â  Â  Â  if (event.error !== 'no-speech' && event.error !== 'aborted') {
Â  Â  Â  Â  Â  Â  // è‡´å‘½çš„ã§ãªã„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å†ã‚¹ã‚¿ãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
Â  Â  Â  Â  Â  Â  restartRecognition();
Â  Â  Â  Â  } else if (event.error === 'no-speech' || event.error === 'network') {
Â  Â  Â  Â  Â  Â  // ç„¡éŸ³ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚å†ã‚¹ã‚¿ãƒ¼ãƒˆ
Â  Â  Â  Â  Â  Â  transcriptBox.textContent = '';
Â  Â  Â  Â  Â  Â  input.value = 'è©±ã—ã‹ã‘ã¦ãã ã•ã„...';
Â  Â  Â  Â  Â  Â  restartRecognition();
Â  Â  Â  Â  }
Â  Â  };

Â  Â  // æœ€åˆã®ã‚¹ã‚¿ãƒ¼ãƒˆ
Â  Â  try {
Â  Â  Â  Â  recognition.start();
Â  Â  } catch (e) {
Â  Â  Â  Â  console.warn('Initial recognition start failed:', e);
Â  Â  }
}

/**
 * ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¦æ±‚ã—ã€AudioContextã€æ³¢å½¢åˆ†æã€ãŠã‚ˆã³STTã‚’è¨­å®šã™ã‚‹
 */
async function initAudioAndSTT(){
Â  Â  // æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯STTã ã‘å†ã‚¹ã‚¿ãƒ¼ãƒˆ
Â  Â  if(analyser) {
Â  Â  Â  Â  startBrowserRecognition();
Â  Â  Â  Â  return;
Â  Â  }
Â  Â  status('Requesting microphone access...');

Â  Â  try {
Â  Â  Â  Â  // 1. AudioContextã®åˆæœŸåŒ–
Â  Â  Â  Â  audioContext = new (window.AudioContext || window.webkitAudioContext)();
Â  Â  Â  Â  analyser = audioContext.createAnalyser();
Â  Â  Â  Â  analyser.fftSize = 2048;
Â  Â  Â  Â  
Â  Â  Â  Â  // 2. ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
Â  Â  Â  Â  mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
Â  Â  Â  Â  const sourceNode = audioContext.createMediaStreamSource(mediaStream);
Â  Â  Â  Â  
Â  Â  Â  Â  // 3. æ¥ç¶šï¼ˆã‚½ãƒ¼ã‚¹ -> ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ï¼‰
Â  Â  Â  Â  sourceNode.connect(analyser);

Â  Â  Â  Â  // 4. ãƒ–ãƒ©ã‚¦ã‚¶STTã®é–‹å§‹
Â  Â  Â  Â  startBrowserRecognition();

Â  Â  Â  Â  status('Listening...');
Â  Â  } catch (e) {
Â  Â  Â  Â  console.error('Audio initialization failed:', e);
Â  Â  Â  Â  status('Error: Microphone access denied or failed to initialize.');
Â  Â  }
}


/* ---------- çµ±åˆã•ã‚ŒãŸãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•° (IoT or LLM) ---------- */

async function processRecognitionResult(finalPrompt) {
Â  Â  // 1. IoTã‚³ãƒãƒ³ãƒ‰ã®åˆ¤å®šã¨æŒ¯ã‚Šåˆ†ã‘
Â  Â  const lowerPrompt = finalPrompt.toLowerCase();
Â  Â  let iotCommand = null;

Â  Â  if ((lowerPrompt.includes('ãƒ©ã‚¤ãƒˆ') || lowerPrompt.includes('é›»æ°—')) && (lowerPrompt.includes('ã¤ã‘') || lowerPrompt.includes('ã‚ªãƒ³'))) {
Â  Â  Â  Â  iotCommand = 'ON';
Â  Â  } else if ((lowerPrompt.includes('ãƒ©ã‚¤ãƒˆ') || lowerPrompt.includes('é›»æ°—')) && (lowerPrompt.includes('ã‘ã—') || lowerPrompt.includes('ã‚ªãƒ•'))) {
Â  Â  Â  Â  iotCommand = 'OFF';
Â  Â  }

Â  Â  if (iotCommand) {
Â  Â  Â  Â  // IoTã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ (ãƒ€ãƒŸãƒ¼å®Ÿè£…)
Â  Â  Â  Â  status(`Executing IoT command: ${iotCommand}... (DUMMY)`);
Â  Â  Â  Â  const message = `æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚${iotCommand === 'ON' ? 'é›»æ°—ã‚’ã¤ã‘ã¾ã—ãŸ' : 'é›»æ°—ã‚’æ¶ˆã—ã¾ã—ãŸ'}ã€‚ï¼ˆã“ã‚Œã¯ãƒ‡ãƒ¢å¿œç­”ã§ã™ï¼‰`;
Â  Â  Â  Â  speak(message);
Â  Â  Â  Â  return; 
Â  Â  }
Â  Â  
Â  Â  // 2. LLMå¿œç­”ç”Ÿæˆï¼ˆIoTã‚³ãƒãƒ³ãƒ‰ã§ãªã‹ã£ãŸå ´åˆï¼‰
Â  Â  await generateAndSpeakResponse(finalPrompt);
}


/* ---------- LLM (Gemini) API & TTS é€£æº ---------- */
async function generateAndSpeakResponse(prompt) {
Â  Â  status('Generating response (Gemini)...');
Â  Â  
Â  Â  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€ŒAIå¿œç­”:ã€ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆã‚’è©±ã—ãŸå ´åˆã€ãã‚Œã‚’é™¤å¤–ã™ã‚‹
Â  Â  const cleanedPrompt = prompt.replace(/^AIå¿œç­”:\s*/, '').trim();
Â  Â  if (!cleanedPrompt) {
Â  Â  Â  Â  return; 
Â  Â  }

Â  Â  const systemInstruction = "ã‚ãªãŸã¯ã€Œã‚¤ãƒã‚¸ãƒ³ã•ã‚“ AIã€ã¨ã„ã†åå‰ã®ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªéŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ã€ç°¡æ½”ã‹ã¤ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚";

Â  Â  const payload = {
Â  Â  Â  Â  contents: [{ parts: [{ text: cleanedPrompt }] }],
Â  Â  Â  Â  systemInstruction: { parts: [{ text: systemInstruction }] },
Â  Â  Â  Â  tools: [{ "google_search": {} }], 
Â  Â  };
Â  Â  
Â  Â  const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_MODEL}:generateContent?key=${API_KEY}`;
Â  Â  let responseText = "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚AIã®å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚";

Â  Â  try {
Â  Â  Â  Â  const response = await fetch(apiUrl, {
Â  Â  Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  Â  Â  headers: { 'Content-Type': 'application/json' },
Â  Â  Â  Â  Â  Â  body: JSON.stringify(payload)
Â  Â  Â  Â  });
Â  Â  Â  Â  
Â  Â  Â  Â  if (!response.ok) {
Â  Â  Â  Â  Â  Â  throw new Error(`HTTP Error! Status: ${response.status}`);
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  const result = await response.json();
Â  Â  Â  Â  const candidate = result.candidates?.[0];

Â  Â  Â  Â  if (candidate && candidate.content?.parts?.[0]?.text) {
Â  Â  Â  Â  Â  Â  responseText = candidate.content.parts[0].text;
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  } catch (e) {
Â  Â  Â  Â  console.error("Gemini API error:", e);
Â  Â  Â  Â  // ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
Â  Â  }

Â  Â  status('Speaking response...');
Â  Â  speak(responseText); 
}

/* ---------- TTS (Speech Synthesis) ---------- */
const synth = window.speechSynthesis;

function speak(text){ 
Â  Â  if(!text) return; 
Â  Â  
Â  Â  // é€£ç¶šå‘¼ã³å‡ºã—æŠ‘åˆ¶ã¨ã‚­ãƒ£ãƒ³ã‚»ãƒ«å‡¦ç†
Â  Â  if(synth.speaking) synth.cancel(); 
Â  Â  
Â  Â  // å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’ transcriptBox ã«è¡¨ç¤º
Â  Â  transcriptBox.textContent = "AIå¿œç­”: " + text;
Â  Â  input.value = "AIå¿œç­”ä¸­...";

Â  Â  const u = new SpeechSynthesisUtterance(text); 
Â  Â  u.lang='ja-JP'; 
Â  Â  u.rate=1.0; 
Â  Â  u.onstart=()=>{ 
Â  Â  Â  Â  status('Speaking...'); 
Â  Â  Â  Â  input.value = "AIå¿œç­”ä¸­...";
Â  Â  }; 
Â  Â  u.onend=()=>{ 
Â  Â  Â  Â  // èª­ã¿ä¸Šã’çµ‚äº†å¾Œã€STTã®onendãƒ­ã‚¸ãƒƒã‚¯ã§å†ã‚¹ã‚¿ãƒ¼ãƒˆã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯çŠ¶æ…‹ã‚’æˆ»ã™
Â  Â  Â  Â  status('Ready to listen...'); 
Â  Â  Â  Â  input.value = "è©±ã—ã‹ã‘ã¦ãã ã•ã„...";
Â  Â  }; 
Â  Â  u.onerror = (e) => {
Â  Â  Â  Â  console.error('TTS error:', e);
Â  Â  Â  Â  status('TTS Error. Ready to listen...');
Â  Â  Â  Â  input.value = "è©±ã—ã‹ã‘ã¦ãã ã•ã„...";
Â  Â  };

Â  Â  synth.speak(u); 
}


/* ---------- UI ãƒˆã‚°ãƒ«æ©Ÿèƒ½ (ç”»é¢ã‚¿ãƒƒãƒ—) ---------- */
let uiVisible = true;
tapArea.addEventListener('click', (e) => {
Â  Â  // è¨˜å…¥æ¬„ã‚„ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã¸ã®ã‚¿ãƒƒãƒ—ã¯ç„¡è¦–ã™ã‚‹
Â  Â  if (e.target.closest('#controls') || e.target.closest('#transcript')) {
Â  Â  Â  Â  return;
Â  Â  }

Â  Â  uiVisible = !uiVisible;
Â  Â  if (uiVisible) {
Â  Â  Â  Â  ui.style.opacity = 1; // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³
Â  Â  } else {
Â  Â  Â  Â  ui.style.opacity = 0; // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆ
Â  Â  }
});


/* ---------- Controls ---------- */
resetBtn.addEventListener('click', ()=>{ 
Â  Â  // STTã¨TTSã‚’å¼·åˆ¶åœæ­¢
Â  Â  if (recognition) {
Â  Â  Â  Â  recognition.stop();
Â  Â  Â  Â  recognition = null;
Â  Â  }
Â  Â  if(synth.speaking) synth.cancel(); 

Â  Â  input.value=''; 
Â  Â  transcriptBox.textContent=''; 
Â  Â  
Â  Â  // å†åº¦éŒ²éŸ³ã‚’é–‹å§‹
Â  Â  initAudioAndSTT();
Â  Â  status('Reset. Listening...'); 
});


/* ---------- Start-up ---------- */
window.onload = function() {
Â  Â  // 1. èµ·å‹•æ™‚ã«ãƒã‚¤ã‚¯åˆæœŸåŒ–ã¨STTã‚’è‡ªå‹•ã§é–‹å§‹
Â  Â  initAudioAndSTT();

Â  Â  // 2. Waveformã®è¦–è¦šãƒ«ãƒ¼ãƒ—
Â  Â  (function loopCanvas(){
Â  Â  Â  Â  if(analyser){
Â  Â  Â  Â  Â  Â  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®æ³¢å½¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
Â  Â  Â  Â  Â  Â  const bufferLen = analyser.frequencyBinCount; 
Â  Â  Â  Â  Â  Â  const data = new Float32Array(bufferLen);
Â  Â  Â  Â  Â  Â  analyser.getFloatTimeDomainData(data); 

Â  Â  Â  Â  Â  Â  ctx.clearRect(0,0,canvas.width,canvas.height);
Â  Â  Â  Â  Â  Â  ctx.beginPath();
Â  Â  Â  Â  Â  Â  const mid = canvas.height*0.55; 
Â  Â  Â  Â  Â  Â  const step = canvas.width / bufferLen;
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  let x = 0; 
Â  Â  Â  Â  Â  Â  for(let i=0;i<bufferLen;i++){ 
Â  Â  Â  Â  Â  Â  Â  Â  // èªè­˜ä¸­ã‹ã©ã†ã‹ã§æŒ¯å¹…ã‚’èª¿æ•´
Â  Â  Â  Â  Â  Â  Â  Â  const amp = isRecording ? 0.9 : 0.2; 
Â  Â  Â  Â  Â  Â  Â  Â  const y=mid + data[i]*mid*amp; 
Â  Â  Â  Â  Â  Â  Â  Â  x = i * step; 
Â  Â  Â  Â  Â  Â  Â  Â  if(i===0)ctx.moveTo(x,y);else ctx.lineTo(x,y) 
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  ctx.strokeStyle='rgba(0,230,255,0.9)'; ctx.lineWidth=2; ctx.stroke();
Â  Â  Â  Â  }
Â  Â  Â  Â  requestAnimationFrame(loopCanvas);
Â  Â  })();
};
</script></body>
</html>
